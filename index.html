<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Group-wise Temporal Logit Adjustment</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Context-Enhanced Memory-Refined Transformer for Online Action Detection</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Zhanzhong Pang</a><sup>1</sup>,  </span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Fadime Sener</a><sup>2</sup>,  </span>
                    <span class="author-block">
                         <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Angela Yao</a><sup>1</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">National University of Singapore<sup>1</sup>,   Meta Reality Labs<sup>2</sup>
                      <br> CVPR2025 </span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="static/pdfs/paper.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/pangzhan27/CMeRT" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Online Action Detection (OAD) detects actions in streaming videos using past observations. State-of-the-art OAD approaches model past observations and their interactions with an anticipated future. The past is encoded using shortand long-term memories to capture immediate and longrange dependencies, while anticipation compensates for missing future context. We identify a training-inference discrepancy in existing OAD methods that hinders learning effectiveness. The training uses varying lengths of shortterm memory, while inference relies on a full-length shortterm memory. As a remedy, we propose a Context-enhanced Memory-Refined Transformer (CMeRT). CMeRT introduces a context-enhanced encoder to improve frame representations using additional near-past context. It also features a memory-refined decoder to leverage near-future generation to enhance performance. CMeRT achieves state-of-theart in online detection and anticipation on THUMOS’14, CrossTask, and EPIC-Kitchens-100.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Online Action Detection</h2>
        <div class="content has-text-justified">
          <p>
              SOTA Online Action Detection is structured in terms of three time zones: the past, the present, and the future. The past offers long-term context that happened a while ago but still matters. The present  captures short-term of recent observations, which are most directly relevant to the ongoing action. The future denotes upcoming frames that are not yet visible but often used as anticipation of what might happen next.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/baseline.png" alt="MY ALT TEXT" width="600" height="700"/>
      <h2 class="subtitle has-text-centered">
      </h2>
    </div>
  </div>
</section>

<!-- End paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Diagnosing Context Modeling</h2>
        <div class="content has-text-justified">
          <p>
            A training-inference mismatch introduces two biases that hinder accurate modeling of the latest frame, which is crucial for inference. First, the causal mask in self-attention creates context imbalance across short-term frames: the latest frame has full context, while the earliest has none. This leads to poor representations for early frames, increasing their loss and degrading the model’s ability to predict the latest frame. Second, using anticipation as pseudo-future introduces non-causal leakage—early frames indirectly access future context, favoring intermediate frames. This results in a valley-shaped loss curve and biased learning that harms final-frame prediction.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/diagnose.png" alt="MY ALT TEXT" width="700" height="800"/>
      <h2 class="subtitle has-text-centered">
      </h2>
    </div>
  </div>
</section>
<!-- Teaser image-->

<!-- End teaser video -->

<!-- Paper method -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methods</h2>
        <div class="content has-text-justified">
          <p>
            CMeRT introduces two extra memories; 1) the near-past, that supplement the context for earlier frames in short-term memory. 2) generated near-future that serves as pseudo-future and avoids information leakage. Unlike standard anticipation, which only serves as near-future for the latest frame but acts as distant future for earlier ones, the near-future we designed overlaps and extends beyond the short memory to serve a pseudo near-future for all short-term frames. Based on these two extra context, CMeRT includes a context-enhanced encoder, and a memory-refined decoder. The context-enhanced encoder leverages the near-past to supplement the context for earlier frames in short-term memory, improving training and yielding better frame representations.  The memory-refinement decoder enhances short-term memory using generated near-future frames. our anticipated future is derived from long-term memory, preventing non-causal leakage and reducing the learning bias toward intermediate frames
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/cmert.png" alt="MY ALT TEXT" width="700" height="800"/>
      <h2 class="subtitle has-text-centered">
      </h2>
    </div>
  </div>
</section>

<!-- End paper method -->

<!-- Paper results -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Evaluation</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate our method on three benchmark datasets using frame-wise, point-wise, and segment-wise metrics. The results show consistent improvements across all metrics, while maintaining efficient runtime performance. To further demonstrate robustness, we also test our approach with advanced DINOv2 features. Based on these findings, we propose a new benchmark for online action detection, including updated datasets, features, and evaluation protocols. Additionally, we introduce a new baseline for online action detection with latency constraints.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/result.png" alt="MY ALT TEXT" width="750" height="850"/>
      <h2 class="subtitle has-text-centered">
      </h2>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/quality_ct.png" alt="MY ALT TEXT" width="780" height="880"/>
      <h2 class="subtitle has-text-centered">
      </h2>
    </div>
  </div>
</section>
<!-- End paper method -->

<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/4Mck7RPxgHM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{pang2025cmert,
        title={Context-Enhanced Memory-Refined Transformer for Online Action Detection},
        author={Zhanzhong Pang and Fadime Sener and Angela Yao},
        journal={CVPR},
        year={2025}}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
